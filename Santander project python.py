# -*- coding: utf-8 -*-
"""my.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GW0lRubwYoH-cNlu2Au6hboXS8d7oOOG

# **Santander-Customer-Transaction-Prediction**
# **Background**:
At Santander, mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals. Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as:

**is a customer satisfied?**


**Will a customer buy this product?**

**Can a customer pay this loan?**

# **Problem Statement:**
We need to identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted.

Supervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features
Classification: The label is a binary variable, 0 (will not make a specific transaction in the future), 1 (will make a specific transaction in the future)

# **Importing libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# data visualisation and manipulation
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from matplotlib import style
from scipy import stats
import seaborn as sns
import missingno as msno
import pandas_profiling as pp
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
# % matplotlib inline  
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
#classification.
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

#evaluation metrics
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification
from sklearn.metrics import classification_report,roc_auc_score,roc_curve, auc,confusion_matrix

"""# **Data Collection**"""

#uncomment below section if deploying this project from Google Colab
#train=pd.read_csv('train.csv')
#test=pd.read_csv('test.csv')


#in this directory i have kept the train.csv and test.csv
os.chidr("C:\users\Rajat\DataScienceProject\")
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
print(os.getcwd())

Profile_report=pp.ProfileReport(train.iloc[:,2:4])
# view profile report generated in the saved repository as a html file
Profile_report.to_file("profile_dataset.html")
print(test.shape)
print(train.shape)
test.head(5)
test.info()

"""####  train contains:

- ID_code (string)
- target
- 200 numerical variables, named from var_0 to var_199
"""

print(test.shape)
print(test.head(5))

test.info()

"""####  test contains:

- ID_code (string)
- 200 numerical variables, named from var_0 to var_199

# **Data Visualizaton**
"""

f,ax=plt.subplots(1,2,figsize=(15,8))

train['target'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.2f%%',ax=ax[0],shadow=True)
ax[0].set_title('Training Set Target Distribution')
ax[0].set_ylabel('')
sns.countplot('target',data=train,ax=ax[1])
plt.show()
# The data is unbalanced with respect to target value

# KDE(Kernal Density Estimate) plots of features with respect to target value 0 and 1 for all variables ie. var_0 to var_199

def plot_feature_distribution(df1, df2, label1, label2, features):
    i = 0
    sns.set_style('whitegrid')
    plt.figure()
    fig, ax = plt.subplots(10,5,figsize=(18,24))

    for feature in features:
        i += 1
        plt.subplot(10,5,i)
        sns.distplot(df1[feature], hist=False,label=label1,kde_kws = {'shade': True, 'linewidth': 2})
        sns.distplot(df2[feature], hist=False,label=label2,kde_kws = {'shade': True, 'linewidth': 2})
        plt.xlabel(feature, fontsize=11)
        locs, labels = plt.xticks()
        plt.tick_params(axis='x', labelsize=6, pad=-6)
        plt.tick_params(axis='y', labelsize=6)
    plt.show()

t0 = train.loc[train['target'] == 0]
t1 = train.loc[train['target'] == 1]
features = train.columns.values[2:52]
plot_feature_distribution(t0, t1, '0', '1', features)
#From var_0 to var_49

features = train.columns.values[52:102]
plot_feature_distribution(t0, t1, '0', '1', features)
#From var_50 to var_99

features = train.columns.values[102:152]
plot_feature_distribution(t0, t1, '0', '1', features)
#From var_100 to var_149

features = train.columns.values[152:202]
plot_feature_distribution(t0, t1, '0', '1', features)
#From var_150 to var_199

"""#### inference from the KDE plots above

- if we look closely var_2, var_9, var_12, var_13, var_26, var_40, var_53, var_81 and many others have resemblance of a dimodal type distribution (having two peaks).
- All of these variables have a bump of frequency that matches the rising of the probability of making a transaction.
- if pdf(target = 1) - pdf(target = 0) > 0, then there is a high probability of the client making a transfer.

# **Missing Value Analysis**
"""

# Commented out IPython magic to ensure Python compatibility.
import missingno as msno
# %matplotlib inline
msno.matrix(train)
#msno.bar(train)
#msno.heatmap(train)

# Missing Value Analysis
obs = train.isnull().sum().sort_values(ascending = False)
percent = round(train.isnull().sum().sort_values(ascending = False)/len(train)*100, 2)
pd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])

# No missing values were found

for col in train.columns[2:]:
    print("Number of unique values of {} : {}".format(col, train[col].nunique()))
    
#Most features have more than thousands of values for each variable except var_68 ie. (451)

train['var_68'].value_counts()

"""### **Checking Duplicate Values in a variable**"""

features = train.columns.values[2:202]
unique_max_train = []
for feature in features:
    values = train[feature].value_counts()
    unique_max_train.append([feature, values.max(), values.idxmax()])

np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).
            sort_values(by = 'Max duplicates', ascending=False).head(15))

"""**Inference from Dulicate value Analysis**

*   var_68, var_108,var_126 and var_12 were the top 4 features with max duplicate values

# **Correlation Analysis**
"""

train_corr = train.corr()
plt.figure(figsize=(12,10))
sns.heatmap(train_corr)

"""From the above heatmap all the variables are not correlated and hence are independent.

**Checking correlation wih Target variable**
"""

corr = train.corr()
abs(corr['target']).sort_values(ascending=False).head(30)

"""# **Outlier Analysis**"""

def plot_feature_boxplot(df, features):
    i = 0
    sns.set_style('whitegrid')
    plt.figure()
    fig, ax = plt.subplots(10,5,figsize=(18,24))

    for feature in features:
        i += 1
        plt.subplot(10,5,i)
        sns.boxplot(df[feature]) 
        plt.xlabel(feature, fontsize=11)
        locs, labels = plt.xticks()
        plt.tick_params(axis='x', labelsize=6, pad=-6)
        plt.tick_params(axis='y', labelsize=6)
    plt.show()

features = train.columns.values[2:52]
plot_feature_boxplot(train, features)
#From var_0 to var_49

features = train.columns.values[52:102]
plot_feature_boxplot(train, features)
#From var_50 to var_99

features = train.columns.values[102:152]
plot_feature_boxplot(train, features)
#From var_100 to var_149

features = train.columns.values[152:202]
plot_feature_boxplot(train, features)
#From var_150 to var_199

# Detect outliers from IQR
Q1 = train.quantile(0.25)
Q3 = train.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

print("df.shape:",train.shape)
df_in = train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]
df_out = train[((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]
print("df_in.shape:",df_in.shape)
print("df_out.shape:",df_out.shape)

df_out['target'].value_counts()

#df_out['target'].value_counts()
train['target'].value_counts()
# comparing the 'train' and 'df_out' dataset,
# we can say that all the data points with target equals to 1 are present as outliers

"""# **Principal component analysis (PCA)**

- PCA is a dimensionality reduction technique that reduces less-informative 'noise' features. 
- But PCA is sensitive to variance and different scales, so standardizing will help PCA perform better.
- However, since we found that the correlation between different features in the training dataset is not that significant, so using PCA might not be meaningful.
"""

from sklearn.preprocessing import StandardScaler

#scaling training data between -1 to +1
standardized_train = StandardScaler().fit_transform(train.set_index(['ID_code','target']))

standardized_train

standardized_train = pd.DataFrame(standardized_train, columns=train.set_index(['ID_code','target']).columns)
standardized_train = train[['ID_code','target']].join(standardized_train)

standardized_train.head(10)

from sklearn.decomposition import PCA
k=80
pca = PCA(n_components=k, random_state=42, whiten=True)
pca.fit(standardized_train.set_index(['ID_code','target']))

sum(pca.explained_variance_ratio_)

plt.figure(figsize=(26,9))
plt.plot(pca.explained_variance_ratio_)
plt.xticks(range(k))
plt.xlabel("Number of Features")
plt.ylabel("Proportion of variance explained by additional feature")

"""Normally, if there is a elbow looking point in the graph above, the x value(number of features) of that point is usually the ideal number of components for PCA.

However in this case, each principal component explains very little of the total variance (e.g. first principal component only explains about 0.6% of the total variance).
Even when we sum up all the variance explained by the 80 principal components, it only amounts to 40%. Let's increase the k and see what happens
"""

sum(PCA(n_components=120, random_state=42, whiten=True).fit(standardized_train.set_index(['ID_code','target'])).\
explained_variance_ratio_)

sum(PCA(n_components=170, random_state=42, whiten=True).fit(standardized_train.set_index(['ID_code','target'])).\
explained_variance_ratio_)

"""- Even with using 170 principal components, 85% of the total variance is explained.
- Hence we proved, that PCA is best when the dimension is very large and a lot of features are correlated to one another a lot.

Hence in our case we can't reduce the dimension of our dataset using PCA.

# **Feature Engineering**
"""

#Created new columns with the unique values count
for Oldvar in ['var_' + str(x) for x in range(200)]:
    train_count_values = train.groupby(Oldvar)[Oldvar].count()
    test_count_values = test.groupby(Oldvar)[Oldvar].count()
    train['new_' + Oldvar] = train_count_values.loc[train[Oldvar]].values
    test['new_' + Oldvar] = test_count_values.loc[test[Oldvar]].values

train.head(3)

test.head(3)

"""# **Modelling**"""

Target = train['target']

# Input dataset for Train and Test 
train_inp = train.drop(columns = ['target', 'ID_code'])
test_inp = test.drop(columns = ['ID_code'])

# List of feature names
features = list(train_inp.columns)

X_train, X_test, y_train, y_test = train_test_split(train_inp, Target, test_size= 0.3, random_state = 2019)

# check the split of train and validation
print('Train:',X_train.shape)
print('Test:',X_test.shape)

"""#### Logistic Regression
- We start with most basic algorithm used for classification problems. 
- Since this is an unbalanced dataset, we need to define parameter 'class_weight = balanced' which will give equal weights to both the targets irrespective of their representation in the training dataset.
"""

#Building Model for Logistic Regression

steps = [('scaler', StandardScaler()),
        ('logreg', LogisticRegression(class_weight='balanced'))]
        
# Create the pipeline: pipeline
pipeline = Pipeline(steps)

# Fit the pipeline to the training set
logreg_scaled = pipeline.fit(X_train,y_train)

y_pred = logreg_scaled.predict_proba(X_test)[:,1]

## first make a model function for modeling with confusion matrix
def model(model,features_train,features_test,labels_train,labels_test):
    clf= model
    clf.fit(features_train,labels_train)
    pred=clf.predict(features_test)
    cnf_matrix=confusion_matrix(labels_test,pred)
    print("the recall for this model is :",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))
    print("the precision for this model is :",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1]))
    fig= plt.figure(figsize=(10,7))
    print("TP",cnf_matrix[1,1]) # no of true transactions which are predicted as true 
    print("TN",cnf_matrix[0,0]) # no of false transaction which are predicted as false 
    print("FP",cnf_matrix[0,1]) # no of false transactions which are predicted as true
    print("FN",cnf_matrix[1,0]) # no of true transactions which are predicted as false 
    sns.heatmap(cnf_matrix,cmap="Blues",annot=True,fmt="d",linewidths=1,linecolor='black')
    plt.title("Confusion_matrix")
    plt.xlabel("Predicted_class")
    plt.ylabel("Real class")
    plt.show()
    print("\n----------Classification Report------------------------------------")
    print(classification_report(labels_test,pred))

#Model Excecution for Logistic Regression
model(logreg_scaled,X_train, X_test, y_train, y_test)

#ROC-AUC

def plot_roc_curve(fpr, tpr):  
    fig= plt.figure(figsize=(8,6))
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    print('(ROC) Curve: ROC_AUC={0:0.5f}'.format(roc_auc_score(y_test, y_pred)))
    plt.title('Receiver Operating Characteristic (ROC) Curve: ROC_AUC={0:0.5f}'.format(roc_auc_score(y_test, y_pred)))
    plt.legend()
    plt.show()

#PR-AUC
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import auc

def plot_precision_recall(y_test, y_pred): 
    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)
# calculate precision-recall AUC
    AUC = auc(recall, precision)
    print('Precision-Recall curve: PR_AUC={0:0.3f}'.format(auc(recall, precision)))
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title(' Precision-Recall curve: PR_AUC={0:0.3f}'.format( auc(recall, precision)))
# plot the precision-recall curve for the model
    plt.plot(recall, precision, marker='.')
# show the plot
    plt.show()

# PR-AUC for Logistic Regression
plot_precision_recall(y_test, y_pred)

# ROC-AUC for Logistic Regression
y_pred = logreg_scaled.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred) 
plot_roc_curve(fpr, tpr)

#Summary for Logistic Regression

#PR score of 0.516, and ROC_AUC score of 0.86227

"""### Decision Trees
- Moving on to a slightly advanced algorithm, decision trees. Again, the parameters here are class_weight to deal with unbalanced target variable, random_state for reproducability of same trees. The feature max_features and min_sample_leaf are used to prune the tree and avoid overfitting to the training data.

- Max_features defines what proportion of available input features will be used to create tree.

- Min_sample_leaf restricts the minimum number of samples in a leaf node, making sure none of the leaf nodes has less than 80 samples in it. If leaf nodes have less samples it implies we have grown the tree too much and trying to predict each sample very precisely, thus leading to overfitting.
"""

#Building Model--DT
tree_clf = DecisionTreeClassifier(class_weight='balanced', random_state = 2019, 
                                  max_features = 0.7, min_samples_leaf = 80)

# Model Execution--DT
model(tree_clf,X_train, X_test, y_train, y_test)

#ROC-AUC for DT

y_pred = tree_clf.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
plot_roc_curve(fpr, tpr)

# PR-AUC for DT

plot_precision_recall(y_test, y_pred)

# Extract feature importances--DT

feature_importance_values = tree_clf.feature_importances_
feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})
feature_importances.sort_values(by='importance', ascending=False).head(n=10)

# SUmmary for DT Model
#PR score of 0.179, and ROC_AUC score of 0.64563

"""### Ensemble Learning
- Ensemble Learning refers to the algorithms that created using ensembles of variour learning algorithms. For example, random forests are ensembles of many decision tree estimators.


- There are 2 types of ensemble learning algorithms 
    1. Bagging Algorithms: Bagging involves having each model in the ensemble vote with equal weight for the final output. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set 
    2. Boosting Algorithms: Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified.

#### Random Forest
- Let's start with building a random forest, with parameters like class_weight, random_state, and hyperparameters like max_features and min_sample_leaf as earlier. We have also defined the n_estimators which is a compulsory parameter. This defines the number of decision trees that will be present in the forest.
"""

# Create random Forest Object using the mentioned parameters
random_forest = RandomForestClassifier(n_estimators=50, random_state=2019, verbose=1,
                                      class_weight='balanced', max_features = 0.5, 
                                       min_samples_leaf = 50)

model(random_forest,X_train, X_test, y_train, y_test)

#ROC-AUC for Random Forest

y_pred = random_forest.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
plot_roc_curve(fpr, tpr)

# PR-AUC for Random Forest
plot_precision_recall(y_test, y_pred)

# Extract feature importances
feature_importance_values = random_forest.feature_importances_
feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})
feature_importances.sort_values(by='importance', ascending=False).head(n=10)

#Summary for Random Forest  
#PR score of 0.318, and ROC_AUC score of 0.78519

"""### Naive **Bayes**

Naive Bayes is a statistical classification technique based on Bayes Theorem. Naive Bayes classifier is the fast, accurate and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets.

Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example, a loan applicant is desirable or not depending on his/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive. This assumption is called class conditional independence.

And our dataset also has no correlation or very less ie. negligible correlation So, Naive bayes algo is perfect for our dataset to build Model.
"""

#Building Model Naive Bayes

#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()

#Executing Naive Bayes Model
model(gnb,X_train, X_test, y_train, y_test)

#ROC-AUC for Naive Bayes

y_pred = gnb.predict_proba(X_test)[:,1]
print(y_pred)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
plot_roc_curve(fpr, tpr)

# PR-AUC for DT

plot_precision_recall(y_test, y_pred)

#Summary for Naive Bayes  
#PR score of 0.318, and ROC_AUC score of 0.6588

"""# **Final Model Selection**

From the above all stats Nave Bayes has highest PR-AUC score ie. 0.584

Hence we are training our model with Nave Bayes and Predicting test dataset using this model.
"""

#Model Excecution for Nave Bayes
gnb = GaussianNB()
model(gnb,X_train, X_test, y_train, y_test)

#predicting test.csv dataset

test_inp = test.drop(columns = ['ID_code'])
predictions = gnb.predict(test_inp)
pred_df = pd.DataFrame(predictions, columns=(['target']))
f_test = test[['ID_code']].join(pred_df)
final_test=f_test.join(test_inp)
print(final_test['target'].value_counts())


final_test.head(15)

# uncomment below code to save file if using google Colab
# writing predicted values to predicted_file.csv 


#from google.colab import files
#final_test.to_csv('predicted_file.csv') 
#files.download('predicted_file.csv')

final_test.to_csv('predicted_file.csv')